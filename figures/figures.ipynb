{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from epicare.envs import EpiCare\n",
    "from epicare.policies import StandardOfCare, ClinicalTrial, Oracle, Random\n",
    "from epicare.evaluations import run_episode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update(bundles.neurips2024())\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "results_directory = \"../algorithms/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Results for Baseline Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 8\n",
    "num_replicates = 4\n",
    "num_episodes = 1000\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with tqdm(total=num_seeds * num_replicates * 4) as pbar:\n",
    "    for seed in range(1, num_seeds + 1):\n",
    "        environment = EpiCare(seed=seed)\n",
    "    \n",
    "        # Setting up the policies\n",
    "        policies = {\n",
    "            \"Random\": Random(environment),\n",
    "            \"SMART\": ClinicalTrial(environment),\n",
    "            \"SoC\": StandardOfCare(environment, alpha=0.8),\n",
    "            \"OP\": Oracle(environment),\n",
    "        }\n",
    "    \n",
    "        # This dictionary will store the stats for each policy\n",
    "        policy_stats = {}\n",
    "    \n",
    "        # Simulation loop\n",
    "        for name, policy in policies.items():\n",
    "            for replicate in range(num_replicates):\n",
    "                for episode in range(num_episodes):\n",
    "                    total_reward, time_to_remission, steps, transitions = (\n",
    "                        run_episode(\n",
    "                            environment, policy, name, policy_stats, verbose=False\n",
    "                        )\n",
    "                    )\n",
    "                    policy_stats[name][\"adverse_event\"][-1] /= steps\n",
    "            \n",
    "                # After simulation, calculate averages and remission rates\n",
    "                average_reward = np.mean(policy_stats[name][\"total_rewards\"]) * 100/64\n",
    "                reward_std = np.std(policy_stats[name][\"total_rewards\"]) * 100/64\n",
    "                remission_rate = np.mean(policy_stats[name][\"remission\"])\n",
    "                average_time_to_remission = (\n",
    "                    np.mean(policy_stats[name][\"times_to_remission\"])\n",
    "                    if policy_stats[name][\"times_to_remission\"]\n",
    "                    else None\n",
    "                )\n",
    "                time_to_remission_std = (\n",
    "                    np.std(policy_stats[name][\"times_to_remission\"])\n",
    "                    if policy_stats[name][\"times_to_remission\"]\n",
    "                    else None\n",
    "                )\n",
    "                adverse_event_rate = np.mean(policy_stats[name][\"adverse_event\"])\n",
    "                adverse_event_rate_std = np.std(policy_stats[name][\"adverse_event\"])\n",
    "                # get sem\n",
    "                adverse_event_rate_sem = adverse_event_rate_std / np.sqrt(num_episodes)\n",
    "        \n",
    "                # Adding data to all_data\n",
    "                all_data.append(\n",
    "                    {\n",
    "                        \"policy\": name,\n",
    "                        \"env_seed\": seed,\n",
    "                        \"mean_return\": average_reward,\n",
    "                        \"std_return\": reward_std,\n",
    "                        \"mean_time_to_remission\": average_time_to_remission,\n",
    "                        \"std_time_to_remission\": time_to_remission_std,\n",
    "                        \"mean_remission_rate\": remission_rate,\n",
    "                        \"mean_adverse_event_rate\": adverse_event_rate,\n",
    "                        \"sem_adverse_event_rate\": adverse_event_rate_sem,\n",
    "                    }\n",
    "                )\n",
    "                pbar.update()\n",
    "\n",
    "# Creating the final dataframe\n",
    "df = pd.DataFrame(all_data)\n",
    "# Saving separate CSV files for each policy\n",
    "for policy in df[\"policy\"].unique():\n",
    "    # make policy name lowercase\n",
    "    policy_df = df[df[\"policy\"] == policy]\n",
    "    policy_df = policy_df.drop(columns=[\"policy\"])\n",
    "    policy_df.to_csv(os.path.join(results_directory, f\"{policy.lower()}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tables\n",
    "\n",
    "### Table 1: Online Evaluation Results - Mean Episeode Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"op\", \"awac\", \"edac\", \"td3_bc\", \"cql\", \"iql\", \"dqn\", \"soc\", \"bc\", \"random\"]\n",
    "baseline = [True, False, False, False, False, False, False, True, False, True]\n",
    "model_names = [\n",
    "    \"OP\",\n",
    "    \"AWAC\",\n",
    "    \"EDAC\",\n",
    "    \"TD3+BC\",\n",
    "    \"CQL\",\n",
    "    \"IQL\",\n",
    "    \"DQN\",\n",
    "    \"SoC\",\n",
    "    \"BC\",\n",
    "    \"Random\",\n",
    "]\n",
    "model_mapping = dict(zip(models, model_names))\n",
    "results = []\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "# Create a dictionary to store the results for each seed and model\n",
    "seed_results = {}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    df = pd.read_csv(os.path.join(results_directory, f\"{model}_results.csv\"))\n",
    "    if not baseline[i]:\n",
    "        df = df[(df['episodes_avail'] == 2**17) & (df['behavior_policy'] == 'smart')]\n",
    "\n",
    "    # Group results by env_seed\n",
    "    grouped = df.groupby(\"env_seed\")\n",
    "\n",
    "    # Calculate mean and standard deviation of mean_return for each seed\n",
    "    for seed, group in grouped:\n",
    "        mean = group[\"mean_return\"].mean()\n",
    "        std = group[\"mean_return\"].std()\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        if seed not in seed_results:\n",
    "            seed_results[seed] = {}\n",
    "        seed_results[seed][model] = (mean, std)\n",
    "\n",
    "    # Calculate overall mean and standard deviation across seeds for the model\n",
    "    overall_mean = df[\"mean_return\"].mean()\n",
    "    overall_std = df.groupby(\"env_seed\")[\"mean_return\"].std().mean()\n",
    "\n",
    "    results.append((model, overall_mean, overall_std))\n",
    "\n",
    "# Sort all models based on overall mean performance\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Reorder to place baseline models first\n",
    "sorted_results = [\n",
    "    res for res in results if model_mapping[res[0]] in [\"OP\", \"SoC\", \"Random\"]\n",
    "] + [res for res in results if model_mapping[res[0]] not in [\"OP\", \"SoC\", \"Random\"]]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "data = {}\n",
    "\n",
    "# Add overall mean and standard deviation for each model\n",
    "mean_row = []\n",
    "for _, mean, std in sorted_results:\n",
    "    if np.isnan(std):\n",
    "        mean_row.append(\"{:.1f}\".format(mean))\n",
    "    else:\n",
    "        mean_row.append(\"{:.1f} ({:.1f})\".format(mean, std))\n",
    "data[\"Mean\"] = mean_row\n",
    "\n",
    "# Add mean and standard deviation for each seed\n",
    "for seed in sorted(seed_results.keys()):\n",
    "    seed_row = []\n",
    "    for model, _, _ in sorted_results:\n",
    "        mean, std = seed_results[seed].get(\n",
    "            model, (0, np.nan)\n",
    "        )  # Default to (0, NaN) if not available\n",
    "        if np.isnan(std):\n",
    "            seed_row.append(\"{:.1f}\".format(mean))\n",
    "        else:\n",
    "            seed_row.append(\"{:.1f} ({:.1f})\".format(mean, std))\n",
    "    data[f\"Seed {seed}\"] = seed_row\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    data, index=[model_mapping[model] for model, _, _ in sorted_results]\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: Online Evaluation Results - Adverse Event Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"op\", \"awac\", \"edac\", \"td3_bc\", \"cql\", \"iql\", \"dqn\", \"soc\", \"bc\", \"random\"]\n",
    "baseline = [True, False, False, False, False, False, False, True, False, True]\n",
    "model_names = [\n",
    "    \"OP\",\n",
    "    \"AWAC\",\n",
    "    \"EDAC\",\n",
    "    \"TD3+BC\",\n",
    "    \"CQL\",\n",
    "    \"IQL\",\n",
    "    \"DQN\",\n",
    "    \"SoC\",\n",
    "    \"BC\",\n",
    "    \"Random\",\n",
    "]\n",
    "model_mapping = dict(zip(models, model_names))\n",
    "results = []\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "# Create a dictionary to store the results for each seed and model\n",
    "seed_results = {}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    df = pd.read_csv(os.path.join(results_directory, f\"{model}_results.csv\"))\n",
    "    if not baseline[i]:\n",
    "        df = df[(df[\"episodes_avail\"] == 2**17) & (df[\"behavior_policy\"] == \"smart\")]\n",
    "\n",
    "    # Group results by env_seed\n",
    "    grouped = df.groupby(\"env_seed\")\n",
    "    # Calculate mean and standard deviation of mean_adverse_event_rate for each seed\n",
    "    for seed, group in grouped:\n",
    "        mean = (\n",
    "            group[\"mean_adverse_event_rate\"].mean() * 10000\n",
    "        )  # Convert percentage to per 10,000\n",
    "        std = (\n",
    "            group[\"mean_adverse_event_rate\"].std() * 10000\n",
    "        )  # Convert percentage to per 10,000\n",
    "        # Store the results in the dictionary\n",
    "        if seed not in seed_results:\n",
    "            seed_results[seed] = {}\n",
    "        seed_results[seed][model] = (mean, std)\n",
    "    # Calculate overall mean and standard deviation across seeds for the model\n",
    "    overall_mean = (\n",
    "        df[\"mean_adverse_event_rate\"].mean() * 10000\n",
    "    )  # Convert percentage to per 10,000\n",
    "    overall_std = (\n",
    "        df.groupby(\"env_seed\")[\"mean_adverse_event_rate\"].std().mean() * 10000\n",
    "    )  # Convert percentage to per 10,000\n",
    "    results.append((model, overall_mean, overall_std))\n",
    "\n",
    "# Sort all models based on overall mean performance\n",
    "results.sort(key=lambda x: -x[1])\n",
    "# Reorder to place baseline models first\n",
    "sorted_results = [\n",
    "    res for res in results if model_mapping[res[0]] in [\"OP\", \"SoC\", \"Random\"]\n",
    "] + [res for res in results if model_mapping[res[0]] not in [\"OP\", \"SoC\", \"Random\"]]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "data = {}\n",
    "\n",
    "# Add overall mean and standard deviation for each model\n",
    "mean_row = []\n",
    "for _, mean, std in sorted_results:\n",
    "    if np.isnan(std):\n",
    "        mean_row.append(\"{:.0f}\".format(mean))\n",
    "    else:\n",
    "        mean_row.append(\"{:.0f} ({:.0f})\".format(mean, std))\n",
    "data[\"Mean\"] = mean_row\n",
    "\n",
    "# Add mean and standard deviation for each seed\n",
    "for seed in sorted(seed_results.keys()):\n",
    "    seed_row = []\n",
    "    for model, _, _ in sorted_results:\n",
    "        mean, std = seed_results[seed].get(\n",
    "            model, (0, np.nan)\n",
    "        )  # Default to (0, NaN) if not available\n",
    "        if np.isnan(std):\n",
    "            seed_row.append(\"{:.0f}\".format(mean))\n",
    "        else:\n",
    "            seed_row.append(\"{:.0f} ({:.0f})\".format(mean, std))\n",
    "    data[f\"Seed {seed}\"] = seed_row\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    data, index=[model_mapping[model] for model, _, _ in sorted_results]\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 6: Online Evalutaion Results - Mean Remission Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"op\", \"awac\", \"edac\", \"td3_bc\", \"cql\", \"iql\", \"dqn\", \"soc\", \"bc\", \"random\"]\n",
    "baseline = [True, False, False, False, False, False, False, True, False, True]\n",
    "model_names = [\n",
    "    \"OP\",\n",
    "    \"AWAC\",\n",
    "    \"EDAC\",\n",
    "    \"TD3+BC\",\n",
    "    \"CQL\",\n",
    "    \"IQL\",\n",
    "    \"DQN\",\n",
    "    \"SoC\",\n",
    "    \"BC\",\n",
    "    \"Random\",\n",
    "]\n",
    "model_mapping = dict(zip(models, model_names))\n",
    "results = []\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "# Create a dictionary to store the results for each seed and model\n",
    "seed_results = {}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    df = pd.read_csv(os.path.join(results_directory, f\"{model}_results.csv\"))\n",
    "    if not baseline[i]:\n",
    "        df = df[(df[\"episodes_avail\"] == 2**17) & (df[\"behavior_policy\"] == \"smart\")]\n",
    "\n",
    "    # Group results by env_seed\n",
    "    grouped = df.groupby(\"env_seed\")\n",
    "\n",
    "    # Calculate mean and standard deviation of mean_remission_rate for each seed\n",
    "    for seed, group in grouped:\n",
    "        mean = group[\"mean_remission_rate\"].mean()\n",
    "        std = group[\"mean_remission_rate\"].std()\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        if seed not in seed_results:\n",
    "            seed_results[seed] = {}\n",
    "        seed_results[seed][model] = (mean, std)\n",
    "\n",
    "    # Calculate overall mean and standard deviation across seeds for the model\n",
    "    overall_mean = df[\"mean_remission_rate\"].mean()\n",
    "    overall_std = df.groupby(\"env_seed\")[\"mean_remission_rate\"].std().mean()\n",
    "\n",
    "    results.append((model, overall_mean, overall_std))\n",
    "\n",
    "# Sort all models based on overall mean performance\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Reorder to place baseline models first\n",
    "sorted_results = [\n",
    "    res for res in results if model_mapping[res[0]] in [\"OP\", \"SoC\", \"Random\"]\n",
    "] + [res for res in results if model_mapping[res[0]] not in [\"OP\", \"SoC\", \"Random\"]]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "data = {}\n",
    "\n",
    "# Add overall mean and standard deviation for each model\n",
    "mean_row = []\n",
    "for _, mean, std in sorted_results:\n",
    "    if np.isnan(std):\n",
    "        mean_row.append(\"{:.2f}\".format(mean))\n",
    "    else:\n",
    "        mean_row.append(\"{:.2f}({:.2f})\".format(mean, std))\n",
    "data[\"Mean\"] = mean_row\n",
    "\n",
    "# Add mean and standard deviation for each seed\n",
    "for seed in sorted(seed_results.keys()):\n",
    "    seed_row = []\n",
    "    for model, _, _ in sorted_results:\n",
    "        mean, std = seed_results[seed].get(\n",
    "            model, (0, np.nan)\n",
    "        )  # Default to (0, NaN) if not available\n",
    "        if np.isnan(std):\n",
    "            seed_row.append(\"{:.2f}\".format(mean))\n",
    "        else:\n",
    "            seed_row.append(\"{:.2f}({:.2f})\".format(mean, std))\n",
    "    data[f\"Seed {seed}\"] = seed_row\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    data, index=[model_mapping[model] for model, _, _ in sorted_results]\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 7: Online Evaluation Results - Time to Remission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"op\", \"awac\", \"edac\", \"td3_bc\", \"cql\", \"iql\", \"dqn\", \"soc\", \"bc\", \"random\"]\n",
    "baseline = [True, False, False, False, False, False, False, True, False, True]\n",
    "model_names = [\n",
    "    \"OP\",\n",
    "    \"AWAC\",\n",
    "    \"EDAC\",\n",
    "    \"TD3+BC\",\n",
    "    \"CQL\",\n",
    "    \"IQL\",\n",
    "    \"DQN\",\n",
    "    \"SoC\",\n",
    "    \"BC\",\n",
    "    \"Random\",\n",
    "]\n",
    "model_mapping = dict(zip(models, model_names))\n",
    "results = []\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "# Create a dictionary to store the results for each seed and model\n",
    "seed_results = {}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    df = pd.read_csv(os.path.join(results_directory, f\"{model}_results.csv\"))\n",
    "    if not baseline[i]:\n",
    "        df = df[(df[\"episodes_avail\"] == 2**17) & (df[\"behavior_policy\"] == \"smart\")]\n",
    "\n",
    "    # Group results by env_seed\n",
    "    grouped = df.groupby(\"env_seed\")\n",
    "\n",
    "    # Calculate mean and standard deviation of mean_time_to_remission for each seed\n",
    "    for seed, group in grouped:\n",
    "        mean = group[\"mean_time_to_remission\"].mean()\n",
    "        std = group[\"mean_time_to_remission\"].std()\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        if seed not in seed_results:\n",
    "            seed_results[seed] = {}\n",
    "        seed_results[seed][model] = (mean, std)\n",
    "\n",
    "    # Calculate overall mean and standard deviation across seeds for the model\n",
    "    overall_mean = df[\"mean_time_to_remission\"].mean()\n",
    "    overall_std = df.groupby(\"env_seed\")[\"mean_time_to_remission\"].std().mean()\n",
    "\n",
    "    results.append((model, overall_mean, overall_std))\n",
    "\n",
    "# Sort all models based on overall mean performance\n",
    "results.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Reorder to place baseline models first\n",
    "sorted_results = [\n",
    "    res for res in results if model_mapping[res[0]] in [\"OP\", \"SoC\", \"Random\"]\n",
    "] + [res for res in results if model_mapping[res[0]] not in [\"OP\", \"SoC\", \"Random\"]]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "data = {}\n",
    "\n",
    "# Add overall mean and standard deviation for each model\n",
    "mean_row = []\n",
    "for _, mean, std in sorted_results:\n",
    "    if np.isnan(std):\n",
    "        mean_row.append(\"{:.1f}\".format(mean))\n",
    "    else:\n",
    "        mean_row.append(\"{:.1f}({:.1f})\".format(mean, std))\n",
    "data[\"Mean\"] = mean_row\n",
    "\n",
    "# Add mean and standard deviation for each seed\n",
    "for seed in sorted(seed_results.keys()):\n",
    "    seed_row = []\n",
    "    for model, _, _ in sorted_results:\n",
    "        mean, std = seed_results[seed].get(\n",
    "            model, (0, np.nan)\n",
    "        )  # Default to (0, NaN) if not available\n",
    "        if np.isnan(std):\n",
    "            seed_row.append(\"{:.1f}\".format(mean))\n",
    "        else:\n",
    "            seed_row.append(\"{:.1f}({:.1f})\".format(mean, std))\n",
    "    data[f\"Seed {seed}\"] = seed_row\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    data, index=[model_mapping[model] for model, _, _ in sorted_results]\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 8: Online Evaluation Results - Behavior Policy Comparison (SoC vs. SMART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"edac\", \"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql\"]\n",
    "model_names = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "model_mapping = dict(zip(models, model_names))\n",
    "results_directory = \"../algorithms/results/\"\n",
    "\n",
    "# Create a dictionary to store the results for each policy type and model\n",
    "policy_results = {\"smart\": {}, \"soc\": {}}\n",
    "\n",
    "for model in models:\n",
    "    for policy in [\"smart\", \"soc\"]:\n",
    "        df = pd.read_csv(os.path.join(results_directory, f\"{model}_results.csv\"))\n",
    "        if policy == \"smart\":\n",
    "            df = df[\n",
    "                (df[\"episodes_avail\"] == 2**17) & (df[\"behavior_policy\"] == \"smart\")\n",
    "            ]\n",
    "        else:\n",
    "            df = df[df[\"behavior_policy\"] == \"soc\"]\n",
    "\n",
    "        # Filter for env_seed == 1\n",
    "        df = df[df[\"env_seed\"] == 1]\n",
    "\n",
    "        # Calculate mean and standard deviation of mean_time_to_remission\n",
    "        mean = df[\"mean_return\"].mean()\n",
    "        std = df[\"mean_return\"].std()\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        if model not in policy_results[policy]:\n",
    "            policy_results[policy][model] = (mean, std)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "data = {\"Model\": model_names}\n",
    "smart_means = []\n",
    "smart_stds = []\n",
    "soc_means = []\n",
    "soc_stds = []\n",
    "\n",
    "for model in models:\n",
    "    mean, std = policy_results[\"smart\"].get(model, (0, np.nan))\n",
    "    smart_means.append(f\"{mean:.1f}\")\n",
    "    smart_stds.append(f\"{std:.1f}\")\n",
    "    mean, std = policy_results[\"soc\"].get(model, (0, np.nan))\n",
    "    soc_means.append(f\"{mean:.1f}\")\n",
    "    soc_stds.append(f\"{std:.1f}\")\n",
    "\n",
    "data[\"SMART Mean (Std)\"] = [\n",
    "    f\"{mean}({std})\" for mean, std in zip(smart_means, smart_stds)\n",
    "]\n",
    "data[\"SoC Mean (Std)\"] = [f\"{mean}({std})\" for mean, std in zip(soc_means, soc_stds)]\n",
    "\n",
    "df_results = pd.DataFrame(data)\n",
    "df_results.set_index(\"Model\", inplace=True)\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: Data Restriction Trials - Median Episode Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [\"cql\", \"iql\", \"dqn\", \"td3_bc\"]  # List of models to plot\n",
    "labels = [\"CQL\", \"IQL\", \"DQN\", \"TD3+BC\"]  # List of labels for the legend\n",
    "\n",
    "# Set the Seaborn color palette to the colorblind palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Adding a vertical line at 2876 patients\n",
    "plt.axvline(x=2876, color=\"C4\", linestyle=\"dotted\")\n",
    "plt.text(2876+200, 13-11, 'Largest Ever SMART', color='C4', rotation=90, verticalalignment='top', fontsize=7)\n",
    "\n",
    "# Adding horizontal lines for typical SoC performance\n",
    "plt.axhline(y=95.7, color=\"k\", linestyle=\"--\", label=\"OP\")\n",
    "plt.axhline(y=44.2, color=\"C5\", linestyle=\"--\", label=\"SoC\")\n",
    "plt.axhline(y=5.5, color=\"gray\", linestyle=\"--\", label=\"Random\")\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "    # Restrict dataframe to only seed 1 and 'smart' behavior policy\n",
    "    df = df[(df['env_seed'] == 1) & (df['behavior_policy'] == 'smart') ]\n",
    "    grouped = df.groupby([\"episodes_avail\"])\n",
    "    median = grouped[\"mean_return\"].median()\n",
    "    min = grouped[\"mean_return\"].min()\n",
    "    max = grouped[\"mean_return\"].max()\n",
    "    std = grouped[\"mean_return\"].std()\n",
    "\n",
    "    # Plotting mean\n",
    "    plt.plot(\n",
    "        median.index, median, marker=\"o\", label=labels[models.index(model)], markersize=4\n",
    "    )\n",
    "\n",
    "    # Adding shaded error (std deviation)\n",
    "    plt.fill_between(median.index, min, max, alpha=0.4)\n",
    "\n",
    "# Setting a logarithmic scale for the x-axis\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Episodes Available\")\n",
    "plt.ylabel(\"Median Episode Reward\")\n",
    "\n",
    "# Making the legend readable and not overlap with the plot\n",
    "plt.legend(title=\"Model/Policy\", loc=\"lower right\")\n",
    "\n",
    "# Fine-tuning the appearance\n",
    "plt.tight_layout()  # Adjusts subplot params so that the subplot(s) fits into the figure area\n",
    "\n",
    "# Saving the figure\n",
    "plt.savefig(\"data_restriction_reward.pdf\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7: Baseline Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 100  # Number of random seeds\n",
    "num_episodes = 1000  # Number of evaluation episodes\n",
    "\n",
    "# Initialize containers for aggregated data\n",
    "all_rewards = []\n",
    "all_remission_rates = []\n",
    "all_times_to_remission = []\n",
    "all_adverse_rates = []\n",
    "num_steps = 0\n",
    "num_transitions = 0\n",
    "\n",
    "with tqdm(total=3 * num_episodes * num_seeds) as pbar:\n",
    "    for seed in range(num_seeds):\n",
    "        environment = EpiCare(seed=seed)\n",
    "\n",
    "        # Setting up the policies\n",
    "        policy_stats = {}\n",
    "        policies = {\n",
    "            \"Random\": Random(environment),\n",
    "            \"SMART\": ClinicalTrial(environment),\n",
    "            \"SoC\": StandardOfCare(environment),\n",
    "        }\n",
    "\n",
    "        # Simulation loop\n",
    "        policy_stats = {}\n",
    "        for name, policy in policies.items():\n",
    "            for episode in range(num_episodes):\n",
    "                total_reward, time_to_remission, steps, transitions = run_episode(\n",
    "                    environment, policy, name, policy_stats, verbose=False\n",
    "                )\n",
    "                policy_stats[name][\"adverse_event\"][-1] /= steps\n",
    "                num_steps += steps\n",
    "                num_transitions += transitions\n",
    "                pbar.update()\n",
    "\n",
    "        # After simulation, calculate averages and remission rates\n",
    "        for stats in policy_stats.values():\n",
    "            stats[\"average_reward\"] = np.mean(stats[\"total_rewards\"])\n",
    "            stats[\"reward_std\"] = np.std(stats[\"total_rewards\"])\n",
    "            stats[\"remission_rate\"] = np.mean(stats[\"remission\"])\n",
    "            stats[\"adverse_rate\"] = np.mean(stats[\"adverse_event\"])\n",
    "            if stats[\"times_to_remission\"]:\n",
    "                stats[\"average_time_to_remission\"] = np.mean(stats[\"times_to_remission\"])\n",
    "            else:\n",
    "                stats[\"average_time_to_remission\"] = None\n",
    "\n",
    "        policy_names = list(policy_stats.keys())\n",
    "\n",
    "        # Process and store data for each seed\n",
    "        df_rewards = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"total_rewards\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_rewards.columns = policy_names\n",
    "        all_rewards.append(df_rewards)\n",
    "\n",
    "        remission_rates = [\n",
    "            np.average(policy_stats[name][\"remission_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_remission_rates.append(remission_rates)\n",
    "        \n",
    "        adverse_rates = [\n",
    "            np.average(policy_stats[name][\"adverse_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_adverse_rates.append(adverse_rates)\n",
    "\n",
    "        df_times_to_remission = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"times_to_remission\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_times_to_remission.columns = policy_names\n",
    "        df_times_to_remission = df_times_to_remission.astype(\"Int64\")\n",
    "        all_times_to_remission.append(df_times_to_remission)\n",
    "\n",
    "# Aggregate data across all seeds\n",
    "agg_rewards = (pd.concat(all_rewards) / 64) * 100\n",
    "agg_remission_rates = pd.DataFrame(all_remission_rates, columns=policy_names)\n",
    "agg_times_to_remission = pd.concat(all_times_to_remission)\n",
    "agg_adverse_rates = pd.DataFrame(all_adverse_rates, columns=policy_names)\n",
    "\n",
    "# Boxplot setup\n",
    "fig, axes = plt.subplots(1, 4, figsize=(6.5, 2.0), width_ratios=[2,1,2,1])\n",
    "\n",
    "# Aggregate plots\n",
    "# Plot 1: Total Reward\n",
    "sns.violinplot(data=agg_rewards, ax=axes[0], bw_adjust=0.1, inner=None)\n",
    "# Mark the mean for each policy\n",
    "for i, name in enumerate(policy_names):\n",
    "    axes[0].plot(i, agg_rewards[name].mean(), \"o\", color=\"black\", markersize=4)\n",
    "axes[0].set_ylabel(\"Total Episode Reward\")\n",
    "\n",
    "# Plot 2: Remission Rate\n",
    "sns.barplot(data=agg_remission_rates, ax=axes[1])\n",
    "axes[1].set_ylabel(\"Remission Rate\")\n",
    "\n",
    "# Plot 3: Time to Remission\n",
    "sns.histplot(\n",
    "    data=agg_times_to_remission, ax=axes[2], multiple=\"dodge\", discrete=True, shrink=0.8\n",
    ")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].yaxis\n",
    "axes[2].set_xlabel(\"Time to Remission\")\n",
    "axes[2].set_xticks(range(1, environment.max_visits + 1))\n",
    "axes[2].axes.yaxis.set_major_formatter(lambda value, text: str(int(value/1000))+\"k\" if value > 0 else \"0\")\n",
    "\n",
    "# Plot 4: Adverse Event Rate\n",
    "sns.barplot(data=agg_adverse_rates, ax=axes[3])\n",
    "axes[3].set_ylabel(\"Adverse Event Rate\")\n",
    "\n",
    "# Fine-tuning the aesthetics for publication quality\n",
    "for ax in axes:\n",
    "    if ax != axes[2]:\n",
    "        ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\")\n",
    "    if ax == axes[1]:\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "# Adjusting the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"compare_policies.pdf\")\n",
    "\n",
    "# Display plots\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Number of transitions per episode: \",\n",
    "    num_transitions / (num_episodes * num_seeds * 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8: Tuning SoC Policy - alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 100  # Number of random seeds\n",
    "num_episodes = 1000  # Number of separate evaluation episodes\n",
    "\n",
    "# Initialize containers for aggregated data\n",
    "all_rewards = []\n",
    "all_remission_rates = []\n",
    "all_times_to_remission = []\n",
    "all_adverse_rates = []\n",
    "num_steps = 0\n",
    "num_transitions = 0\n",
    "\n",
    "with tqdm(total=11*num_episodes*num_seeds) as pbar:\n",
    "    for seed in range(num_seeds):\n",
    "        environment = EpiCare(seed=seed)\n",
    "\n",
    "        # Setting up the policies\n",
    "        policy_stats = {}\n",
    "        # SoC with all alphas between 0 and 1, in increments of 0.1\n",
    "        policies = {\n",
    "            f\"{alpha:.1f}\": StandardOfCare(environment, alpha=alpha)\n",
    "            for alpha in [i * 0.1 for i in range(11)]\n",
    "        }\n",
    "\n",
    "        # Simulation loop\n",
    "        policy_stats = {}\n",
    "        for name, policy in policies.items():\n",
    "            for episode in range(num_episodes):\n",
    "                total_reward, time_to_remission, steps, transitions = run_episode(\n",
    "                    environment, policy, name, policy_stats, verbose=False\n",
    "                )\n",
    "                num_steps += steps\n",
    "                num_transitions += transitions\n",
    "                pbar.update()\n",
    "\n",
    "        # After simulation, calculate averages and remission rates\n",
    "        for stats in policy_stats.values():\n",
    "            stats[\"average_reward\"] = np.mean(stats[\"total_rewards\"])\n",
    "            stats[\"reward_std\"] = np.std(stats[\"total_rewards\"])\n",
    "            stats[\"remission_rate\"] = np.mean(stats[\"remission\"])\n",
    "            stats[\"adverse_rate\"] = np.mean(stats[\"adverse_event\"])\n",
    "            if stats[\"times_to_remission\"]:\n",
    "                stats[\"average_time_to_remission\"] = np.mean(stats[\"times_to_remission\"])\n",
    "            else:\n",
    "                stats[\"average_time_to_remission\"] = None\n",
    "\n",
    "        policy_names = list(policy_stats.keys())\n",
    "\n",
    "        # Process and store data for each seed\n",
    "        df_rewards = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"total_rewards\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_rewards.columns = policy_names\n",
    "        all_rewards.append(df_rewards)\n",
    "\n",
    "        remission_rates = [\n",
    "            np.average(policy_stats[name][\"remission_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_remission_rates.append(remission_rates)\n",
    "        \n",
    "        adverse_rates = [\n",
    "            np.average(policy_stats[name][\"adverse_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_adverse_rates.append(adverse_rates)\n",
    "\n",
    "        df_times_to_remission = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"times_to_remission\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_times_to_remission.columns = policy_names\n",
    "        df_times_to_remission = df_times_to_remission.astype(\"Int64\")\n",
    "        all_times_to_remission.append(df_times_to_remission)\n",
    "\n",
    "# Aggregate data across all seeds\n",
    "agg_rewards = (pd.concat(all_rewards) / 64) * 100\n",
    "agg_remission_rates = pd.DataFrame(all_remission_rates, columns=policy_names)\n",
    "agg_times_to_remission = pd.concat(all_times_to_remission)\n",
    "agg_adverse_rates = pd.DataFrame(all_adverse_rates, columns=policy_names)\n",
    "\n",
    "# Generate a list of alpha values as strings for the x-axis labels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6.5, 2.0))\n",
    "\n",
    "# Plot 1: Total Reward\n",
    "sns.violinplot(\n",
    "    data=agg_rewards, ax=axes[0], bw_adjust=0.1, cut=0.0, inner=None, linewidth=0.5\n",
    ")\n",
    "for i, alpha in enumerate(policies):\n",
    "    axes[0].plot(i, agg_rewards[alpha].mean(), \"o\", color=\"black\", markersize=2)\n",
    "axes[0].set_ylabel(\"Total Episode Reward\")\n",
    "axes[0].set_xlabel(\"$\\\\alpha$\")\n",
    "\n",
    "# Plot 2: Remission Rate\n",
    "sns.barplot(data=agg_remission_rates, ax=axes[1])\n",
    "axes[1].set_ylabel(\"Remission Rate\")\n",
    "axes[1].set_xlabel(\"$\\\\alpha$\")\n",
    "\n",
    "# Plot 3: Time to Remission\n",
    "sns.histplot(\n",
    "    data=agg_times_to_remission.melt(var_name=\"Policy\", value_name=\"Time_to_Remission\"),\n",
    "    x=\"Time_to_Remission\",\n",
    "    hue=\"Policy\",\n",
    "    ax=axes[2],\n",
    "    multiple=\"dodge\",\n",
    "    discrete=True,\n",
    "    shrink=0.9,\n",
    "    linewidth=0.2,\n",
    ")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_xlabel(\"Time to Remission\")\n",
    "axes[2].set_xticks(range(1, environment.max_visits + 1))\n",
    "# remove legend completely\n",
    "axes[2].get_legend().remove()\n",
    "axes[2].axes.yaxis.set_major_formatter(lambda value, text: str(int(value/1000))+\"k\" if value > 0 else \"0\")\n",
    "\n",
    "# Fine-tuning the aesthetics for publication quality\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"x\", labelrotation=0)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\")\n",
    "    if ax == axes[1]:\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "# Adjusting the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"SoC_alpha.pdf\")\n",
    "\n",
    "# Display plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8: Tuning SoC Policy - kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_seeds = 100  # Number of random seeds\n",
    "num_episodes = 10000  # Number of separate evaluation episodes\n",
    "\n",
    "# Initialize containers for aggregated data\n",
    "all_rewards = []\n",
    "all_remission_rates = []\n",
    "all_times_to_remission = []\n",
    "all_adverse_rates = []\n",
    "num_steps = 0\n",
    "num_transitions = 0\n",
    "\n",
    "with tqdm(total=6*num_episodes*num_seeds) as pbar:\n",
    "    for seed in range(num_seeds):\n",
    "        environment = EpiCare(seed=seed)\n",
    "\n",
    "        # Setting up the policies\n",
    "        policy_stats = {}\n",
    "        # SoC with all kappas between 0 and 0.5, in increments of 0.1\n",
    "        policies = {\n",
    "            f\"{kappa:.1f}\": StandardOfCare(environment, kappa=kappa)\n",
    "            for kappa in [i * 0.1 for i in range(6)]\n",
    "        }\n",
    "\n",
    "        # Simulation loop\n",
    "        policy_stats = {}\n",
    "        for name, policy in policies.items():\n",
    "            for episode in range(num_episodes):\n",
    "                total_reward, time_to_remission, steps, transitions = run_episode(\n",
    "                    environment, policy, name, policy_stats, verbose=False\n",
    "                )\n",
    "                policy_stats[name][\"adverse_event\"][-1] /= steps\n",
    "                num_steps += steps\n",
    "                num_transitions += transitions\n",
    "                pbar.update()\n",
    "\n",
    "        # After simulation, calculate averages and remission rates\n",
    "        for stats in policy_stats.values():\n",
    "            stats[\"average_reward\"] = np.mean(stats[\"total_rewards\"])\n",
    "            stats[\"reward_std\"] = np.std(stats[\"total_rewards\"])\n",
    "            stats[\"remission_rate\"] = np.mean(stats[\"remission\"])\n",
    "            stats[\"adverse_rate\"] = np.mean(stats[\"adverse_event\"])\n",
    "            if stats[\"times_to_remission\"]:\n",
    "                stats[\"average_time_to_remission\"] = np.mean(stats[\"times_to_remission\"])\n",
    "            else:\n",
    "                stats[\"average_time_to_remission\"] = None\n",
    "\n",
    "        policy_names = list(policy_stats.keys())\n",
    "\n",
    "        # Process and store data for each seed\n",
    "        df_rewards = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"total_rewards\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_rewards.columns = policy_names\n",
    "        all_rewards.append(df_rewards)\n",
    "\n",
    "        remission_rates = [\n",
    "            np.average(policy_stats[name][\"remission_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_remission_rates.append(remission_rates)\n",
    "        \n",
    "        adverse_rates = [\n",
    "            np.average(policy_stats[name][\"adverse_rate\"]) for name in policy_names\n",
    "        ]\n",
    "        all_adverse_rates.append(adverse_rates)\n",
    "\n",
    "        df_times_to_remission = pd.DataFrame.from_records(\n",
    "            [policy_stats[name][\"times_to_remission\"] for name in policy_names]\n",
    "        ).T\n",
    "        df_times_to_remission.columns = policy_names\n",
    "        df_times_to_remission = df_times_to_remission.astype(\"Int64\")\n",
    "        all_times_to_remission.append(df_times_to_remission)\n",
    "\n",
    "# Aggregate data across all seeds, then plot\n",
    "agg_rewards = (pd.concat(all_rewards) / 64) * 100\n",
    "agg_remission_rates = pd.DataFrame(all_remission_rates, columns=policy_names)\n",
    "agg_times_to_remission = pd.concat(all_times_to_remission)\n",
    "agg_adverse_rates = pd.DataFrame(all_adverse_rates, columns=policy_names)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(6.5, 2.0))\n",
    "\n",
    "# Plot 1: Total Reward\n",
    "sns.violinplot(\n",
    "    data=agg_rewards, ax=axes[0], bw_adjust=0.1, cut=0.0, inner=None, linewidth=0.5\n",
    ")\n",
    "for i, kappa in enumerate(policies):\n",
    "    axes[0].plot(i, agg_rewards[kappa].mean(), \"o\", color=\"black\", markersize=2)\n",
    "axes[0].set_ylabel(\"Total Episode Reward\")\n",
    "axes[0].set_xlabel(\"$\\\\kappa$\")\n",
    "\n",
    "# Plot 2: Remission Rate\n",
    "sns.barplot(data=agg_remission_rates, ax=axes[1])\n",
    "axes[1].set_ylabel(\"Remission Rate\")\n",
    "axes[1].set_xlabel(\"$\\\\kappa$\")\n",
    "\n",
    "# Plot 3: Time to Remission\n",
    "sns.histplot(\n",
    "    data=agg_times_to_remission.melt(var_name=\"Policy\", value_name=\"Time_to_Remission\"),\n",
    "    x=\"Time_to_Remission\",\n",
    "    hue=\"Policy\",\n",
    "    ax=axes[2],\n",
    "    multiple=\"dodge\",\n",
    "    discrete=True,\n",
    "    shrink=0.9,\n",
    "    linewidth=0.2,\n",
    ")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_xlabel(\"Time to Remission\")\n",
    "axes[2].set_xticks(range(1, environment.max_visits + 1))\n",
    "# remove legend completely\n",
    "axes[2].get_legend().remove()\n",
    "axes[2].axes.yaxis.set_major_formatter(lambda value, text: str(int(value/1000))+\"k\" if value > 0 else \"0\")\n",
    "\n",
    "# Plot 4: Adverse Event Rate\n",
    "sns.barplot(data=agg_adverse_rates, ax=axes[3])\n",
    "axes[3].set_ylabel(\"Adverse Events per Step\")\n",
    "axes[3].set_xlabel(\"$\\\\kappa$\")\n",
    "\n",
    "# Fine-tuning the aesthetics for publication quality\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"x\", labelrotation=0)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\")\n",
    "    if ax == axes[1]:\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "# Adjusting the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"SoC_kappa.pdf\")\n",
    "\n",
    "# Display plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11: Data Restriction Trials - Median Adverse Event Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"cql\", \"iql\", \"dqn\", \"td3_bc\"]  # List of models to plot\n",
    "labels = [\"CQL\", \"IQL\", \"DQN\", \"TD3+BC\"]  # List of labels for the legend\n",
    "\n",
    "# Set the Seaborn color palette to the colorblind palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Adding a vertical line at 2876 patients\n",
    "plt.axvline(x=2876, color=\"C4\", linestyle=\"dotted\")\n",
    "plt.text(2876 - 675, 64, 'Largest Ever SMART', color='C4', rotation=90, verticalalignment='bottom', fontsize=8)\n",
    "\n",
    "# Adding horizontal lines for typical SoC performance\n",
    "plt.axhline(y=0, color=\"k\", linestyle=\"--\", label=\"OP\")\n",
    "plt.axhline(y=15, color=\"C5\", linestyle=\"--\", label=\"SoC\")\n",
    "plt.axhline(y=56, color=\"gray\", linestyle=\"--\", label=\"Random\")\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "    # Restrict dataframe to only seed 1 and 'smart' behavior policy\n",
    "    df = df[(df[\"env_seed\"] == 1) & (df[\"behavior_policy\"] == \"smart\")]\n",
    "    grouped = df.groupby([\"episodes_avail\"])\n",
    "    median = grouped[\"mean_adverse_event_rate\"].median()*10000\n",
    "    min = grouped[\"mean_adverse_event_rate\"].min() * 10000\n",
    "    max = grouped[\"mean_adverse_event_rate\"].max() * 10000\n",
    "    std = grouped[\"mean_adverse_event_rate\"].std()\n",
    "\n",
    "    # Plotting mean\n",
    "    plt.plot(\n",
    "        median.index, median, marker=\"o\", label=labels[models.index(model)], markersize=4\n",
    "    )\n",
    "\n",
    "    # Adding shaded error (std deviation)\n",
    "    plt.fill_between(median.index, min, max, alpha=0.4)\n",
    "\n",
    "# Setting a logarithmic scale for the x-axis\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Episodes Available\")\n",
    "plt.ylabel(\"Median Adverse Events per 10k Episodes\")\n",
    "\n",
    "# Making the legend readable and not overlap with the plot\n",
    "plt.legend(title=\"Model/Policy\", loc=\"upper right\")\n",
    "\n",
    "# Fine-tuning the appearance\n",
    "plt.tight_layout()  # Adjusts subplot params so that the subplot(s) fits into the figure area\n",
    "\n",
    "# Saving the figure\n",
    "plt.savefig(\"data_restriction_aer.pdf\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 14: OPE RMSE vs. Action Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OPE methods and models\n",
    "models = [\"edac\", \"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql\"]\n",
    "model_labels = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "ope_methods = [\"is\", \"wis\", \"pdis\", \"wpdis\"]\n",
    "colors = [\"C\" + str(i) for i in range(len(models))]\n",
    "color_dict = dict(zip(models, colors))\n",
    "model_labes_dict = dict(zip(models, model_labels))\n",
    "\n",
    "# Initialize lists to store mean log_probs, their standard deviations, and average RMSE for each model\n",
    "mean_log_probs = []\n",
    "std_devs = []\n",
    "average_rmse_values = []\n",
    "\n",
    "# Process data for each model to get log_probs and calculate RMSE\n",
    "for model in models:\n",
    "    df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "    # Filter rows for checkpoint_32.pt\n",
    "    if \"checkpoint\" in df.columns:\n",
    "        df_filtered = df[df[\"checkpoint\"] == \"checkpoint_31.pt\"]\n",
    "        abs_log_probs = df_filtered[\"mean_meanlogprob_estimate\"].abs()\n",
    "        mean_log_prob = abs_log_probs.mean()\n",
    "        std_dev = abs_log_probs.std()\n",
    "    else:\n",
    "        mean_log_prob = std_dev = np.nan\n",
    "\n",
    "    mean_log_probs.append(mean_log_prob)\n",
    "    std_devs.append(std_dev)\n",
    "\n",
    "    # Calculate RMSE for each model across all OPE methods\n",
    "    rmse_values = []\n",
    "    for method in ope_methods:\n",
    "        online_means = df[\"mean_return\"]\n",
    "        ope_means = df[f\"mean_{method}_estimate\"]\n",
    "        rmse = np.sqrt(\n",
    "            np.mean(\n",
    "                (np.clip(ope_means, -100, 100) - np.clip(online_means, -100, 100)) ** 2\n",
    "            )\n",
    "        )\n",
    "        rmse_values.append(rmse)\n",
    "    average_rmse = np.mean(rmse_values)\n",
    "    average_rmse_values.append(average_rmse)\n",
    "\n",
    "# Combine models, their mean log_probs, std_devs, and average RMSE into a DataFrame for sorting\n",
    "model_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"MeanLogProb\": mean_log_probs,\n",
    "        \"StdDev\": std_devs,\n",
    "        \"AverageRMSE\": average_rmse_values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by AverageRMSE from smallest to largest\n",
    "model_data_sorted = model_data.sort_values(by=\"AverageRMSE\")\n",
    "\n",
    "# Create a bar graph for Average RMSE\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(\n",
    "    model_data_sorted[\"Model\"],\n",
    "    model_data_sorted[\"AverageRMSE\"],\n",
    "    color=[color_dict[model] for model in model_data_sorted[\"Model\"]],\n",
    "    capsize=5,\n",
    ")\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.set_ylabel(\"Average OPE RMSE\", color=\"k\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"k\")\n",
    "ax1.set_xticklabels([model_labes_dict[model] for model in model_data_sorted[\"Model\"]])\n",
    "\n",
    "# Add a secondary y-axis for Mean Abs Log Probability as a line graph\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    model_data_sorted[\"Model\"],\n",
    "    np.e ** -model_data_sorted[\"MeanLogProb\"],\n",
    "    color=\"C\" + str(len(model)),\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    ")\n",
    "ax2.set_ylabel(\n",
    "    \"Average Probability of Training Action\",\n",
    "    color=\"C\" + str(len(model)),\n",
    "    rotation=-90,\n",
    "    labelpad=13,\n",
    ")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"C\" + str(len(model)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"avg_probs_and_rmse.pdf\")  # Save as high-resolution image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope_methods = [\"is\", \"wis\", \"pdis\", \"wpdis\", \"direct\"]\n",
    "ope_labels = [\"IS\", \"WIS\", \"PDIS\", \"WPDIS\", \"DM\"]\n",
    "models = [\"edac\",\"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql-dqn\"]\n",
    "model_labels = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=len(models), nrows=len(ope_methods), figsize=(6.75, 5.5)\n",
    ")  # Adjust size as needed\n",
    "\n",
    "# Initialize a dictionary to store RMSE values for each model\n",
    "model_rmse = {model: [] for model in models}\n",
    "\n",
    "# Process data for each model and each OPE method\n",
    "for col, model in enumerate(models):\n",
    "    for row, method in enumerate(ope_methods):\n",
    "        df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "        online_means = df[\"mean_return\"]\n",
    "        ope_means = df[f\"mean_{method}_estimate\"]\n",
    "\n",
    "        # Clip the values to be between -100 and 100\n",
    "        online_means_clipped = np.clip(online_means, -100, 100)\n",
    "        ope_means_clipped = np.clip(ope_means, -100, 100)\n",
    "\n",
    "        # Calculate RMSE and store it\n",
    "        rmse = np.sqrt(np.mean((online_means - ope_means) ** 2))\n",
    "        model_rmse[model].append(rmse)\n",
    "\n",
    "        # Plot scatter for each method and model\n",
    "        axes[row, col].scatter(online_means_clipped, ope_means_clipped, s=5, alpha=0.5)\n",
    "        axes[row, col].set_aspect(\"equal\", adjustable=\"box\")\n",
    "        axes[row, col].set_xlim([-75, 100])\n",
    "        axes[row, col].set_ylim([-75, 100])\n",
    "        axes[row, col].plot([-75, 100], [-75, 100], color=\"black\")  # y=x line\n",
    "        axes[row, col].set_xticks([])\n",
    "        axes[row, col].set_yticks([])\n",
    "\n",
    "        # Add RMSE as an insert, centered below the scatter plot\n",
    "        axes[row, col].text(\n",
    "            0.5,\n",
    "            1.3,\n",
    "            f\"RMSE: {rmse:0.2f}\",\n",
    "            transform=axes[row, col].transAxes,\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", alpha=1.0, facecolor=\"white\"),\n",
    "        )\n",
    "\n",
    "for row in range(len(ope_methods)):\n",
    "    axes[row, 0].set_ylabel(ope_labels[row] + \" Estimate\")\n",
    "    axes[row, 0].set_yticks([-75, 0, 100])\n",
    "\n",
    "for col in range(len(models)):\n",
    "    axes[0, col].set_title(model_labels[col].upper(), pad=20)\n",
    "    axes[-1, col].set_xlabel(\"Online Return\")\n",
    "    axes[-1, col].xaxis.tick_bottom()\n",
    "    axes[-1, col].set_xticks([-75, 0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ope_scatter.pdf\")  # Save as high-resolution image\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print average RMSE for each model\n",
    "average_rmse = {model: np.mean(rmses) for model, rmses in model_rmse.items()}\n",
    "print(\"Average RMSE for each model:\", average_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope_methods = [\"is\", \"wis\", \"pdis\", \"wpdis\", \"direct\"]\n",
    "ope_labels = [\"IS\", \"WIS\", \"PDIS\", \"WPDIS\", \"DM\"]\n",
    "models = [\"edac\", \"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql-dqn\"]\n",
    "model_labels = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=len(models), nrows=len(ope_methods), figsize=(6.75, 5.5)\n",
    ")  # Adjust size as needed\n",
    "\n",
    "# Initialize a dictionary to store RMSE values for each model\n",
    "model_rmse = {model: [] for model in models}\n",
    "\n",
    "# Process data for each model and each OPE method\n",
    "for col, model in enumerate(models):\n",
    "    for row, method in enumerate(ope_methods):\n",
    "        df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "        online_error = np.abs(df[\"mean_return\"] - df[f\"mean_{method}_estimate\"])\n",
    "        ope_stds = df[f\"std_{method}_estimate\"]\n",
    "\n",
    "        # Plot scatter for each method and model\n",
    "        axes[row, col].scatter(online_error, ope_stds, s=5, alpha=0.5)\n",
    "        axes[row, col].set_xlim([0, 100])\n",
    "        axes[row, col].set_ylim([0, 100])\n",
    "        axes[row, col].set_xticks([])\n",
    "        axes[row, col].set_yticks([])\n",
    "\n",
    "for row in range(len(ope_methods)):\n",
    "    axes[row, 0].set_ylabel(\"Std \" + ope_labels[row] + \" Estimate\")\n",
    "    axes[row, 0].set_yticks([0, 200])\n",
    "\n",
    "for col in range(len(models)):\n",
    "    axes[0, col].set_title(model_labels[col].upper(), pad=20)\n",
    "    axes[-1, col].set_xlabel(\"OPE Error\")\n",
    "    axes[-1, col].xaxis.tick_bottom()\n",
    "    axes[-1, col].set_xticks([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ope_std_scatter.pdf\")  # Save as high-resolution image\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print average RMSE for each model\n",
    "average_rmse = {model: np.mean(rmses) for model, rmses in model_rmse.items()}\n",
    "print(\"Average RMSE for each model:\", average_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope_methods = [\"is\", \"wis\", \"pdis\", \"wpdis\", \"direct\"]\n",
    "ope_labels = [\"IS\", \"WIS\", \"PDIS\", \"WPDIS\", \"DM\"]\n",
    "models = [\"edac\", \"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql-dqn\"]\n",
    "model_labels = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "\n",
    "# Initialize a dictionary to store the standard deviations for each OPE method\n",
    "ope_stds_agg = {method: [] for method in ope_methods}\n",
    "outliers_agg = {method: [] for method in ope_methods}\n",
    "\n",
    "# Process data for each model and each OPE method\n",
    "for model in models:\n",
    "    df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "    for method in ope_methods:\n",
    "        ope_stds = df[f\"std_{method}_estimate\"]\n",
    "        ope_stds_agg[method].extend(ope_stds)\n",
    "        outliers_agg[method].extend(ope_stds[ope_stds > 100])\n",
    "\n",
    "# Determine the common horizontal range with a cutoff at 100\n",
    "min_std, max_std = 0, 100\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(ope_methods), figsize=(6, 8))\n",
    "\n",
    "# Plot histograms for each OPE method\n",
    "for idx, method in enumerate(ope_methods):\n",
    "    axes[idx].hist(\n",
    "        ope_stds_agg[method], bins=30, alpha=0.7, range=(min_std, max_std), log=True\n",
    "    )\n",
    "    axes[idx].set_title(f\"{ope_labels[idx]} Estimate\")\n",
    "    axes[idx].set_xlabel(\"Standard Deviation\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ope_std_histograms.pdf\")  # Save as high-resolution image\n",
    "plt.show()\n",
    "\n",
    "# Print all outliers along with the associated OPE method\n",
    "print(\"Outliers exceeding the range (0, 100):\")\n",
    "for method in ope_methods:\n",
    "    for outlier in outliers_agg[method]:\n",
    "        print(f\"OPE Method: {method.upper()}, Outlier Value: {outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope_methods = [\"is\", \"wis\", \"pdis\", \"wpdis\", \"direct\"]\n",
    "models = [\"edac\", \"awac\", \"bc\", \"td3_bc\", \"iql\", \"dqn\", \"cql-dqn\"]\n",
    "model_labels = [\"EDAC\", \"AWAC\", \"BC\", \"TD3+BC\", \"IQL\", \"DQN\", \"CQL\"]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=len(models), nrows=len(ope_methods), figsize=(6.75, 5.5)\n",
    ")  # Adjust size as needed\n",
    "\n",
    "# Initialize a dictionary to store RMSE values for each model\n",
    "model_rmse = {model: [] for model in models}\n",
    "\n",
    "# Process data for each model and each OPE method\n",
    "for col, model in enumerate(models):\n",
    "    for row, method in enumerate(ope_methods):\n",
    "        df = pd.read_csv(os.path.join(results_directory, model + \"_results.csv\"))\n",
    "        online_means = df[\"mean_return\"]\n",
    "        ope_means = df[f\"mean_{method}_estimate\"]\n",
    "\n",
    "        # Clip the values to be between -100 and 100\n",
    "        #online_means = np.clip(online_means, -100, 100)\n",
    "        #ope_means = np.clip(ope_means, -100, 100)\n",
    "\n",
    "        # Calculate RMSE and store it\n",
    "        corr = np.corrcoef(online_means, ope_means)[0,1]\n",
    "        print(model, method, corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
